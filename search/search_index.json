{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome \u00b6 Currently, I am working as an optimisation engineer in AGCO Randers Innovation Center since the year 2018. I received my PhD from Aarhus University, the research group of Operations Management in 2015. My research focuses on the following topics: \ud83d\udccc Operations planning: mission planning, task scheduling and allocation \ud83d\udccc Field logistics: scheduling, area coverage planning, routing for machines and robots \ud83d\udccc Information engineering: Identifying relevant data sources for business needs, data processing, decision optimisation Route and path planning algorithms that I developed have been applied to the following AGCO products: 1\ufe0f\u20e3 Geo-Bird is an easy-to-use web application focusing on optimizing farming and reducing costs while at the same time improving the CO2 footprint, it is not just your tool for wayline creation but also analyses your field and optimizes the waylines for Controlled Traffic Farming(CTF) to reduce wheel traffic, time and soil compaction.I am responsible for the wayline planning and optimal wayline direction algorithms. (photos courtesy AGCO/Fuse) 2\ufe0f\u20e3 FendtONE Offboard complements the FendtONE onboard on the machine with practical, additional functions. For example, Fendt Task Doc, which records agronomy data/yield figures during field work (onboard), has been expanded with job planning and administration functions in Fendt Task Doc (offboard). My wayline planning algorithm is intergated in the system. (photos courtesy AGCO/Fendt) 3\ufe0f\u20e3 Fendt Xaver is a compact, electric-powered prototype that\u2019s designed to work autonomously in swarms in a field, with the aim of reducing soil compaction, energy consumption, and labour costs. My path and route planning algorithms is integated as a module of the entire robot system. (photos courtesy AGCO/Fendt)","title":"Welcome"},{"location":"#welcome","text":"Currently, I am working as an optimisation engineer in AGCO Randers Innovation Center since the year 2018. I received my PhD from Aarhus University, the research group of Operations Management in 2015. My research focuses on the following topics: \ud83d\udccc Operations planning: mission planning, task scheduling and allocation \ud83d\udccc Field logistics: scheduling, area coverage planning, routing for machines and robots \ud83d\udccc Information engineering: Identifying relevant data sources for business needs, data processing, decision optimisation Route and path planning algorithms that I developed have been applied to the following AGCO products: 1\ufe0f\u20e3 Geo-Bird is an easy-to-use web application focusing on optimizing farming and reducing costs while at the same time improving the CO2 footprint, it is not just your tool for wayline creation but also analyses your field and optimizes the waylines for Controlled Traffic Farming(CTF) to reduce wheel traffic, time and soil compaction.I am responsible for the wayline planning and optimal wayline direction algorithms. (photos courtesy AGCO/Fuse) 2\ufe0f\u20e3 FendtONE Offboard complements the FendtONE onboard on the machine with practical, additional functions. For example, Fendt Task Doc, which records agronomy data/yield figures during field work (onboard), has been expanded with job planning and administration functions in Fendt Task Doc (offboard). My wayline planning algorithm is intergated in the system. (photos courtesy AGCO/Fendt) 3\ufe0f\u20e3 Fendt Xaver is a compact, electric-powered prototype that\u2019s designed to work autonomously in swarms in a field, with the aim of reducing soil compaction, energy consumption, and labour costs. My path and route planning algorithms is integated as a module of the entire robot system. (photos courtesy AGCO/Fendt)","title":"Welcome"},{"location":"home/publications/","text":"Publications \u00b6 \ud83d\udcd7 : Published \ud83d\udcd5 : Under review Journal papers \u00b6 \ud83d\udcd7 Chen, Y., Li, G., Zhang, X., Jia, J., Zhou, K. , & Wu, C. (2022). Identifying field and road modes of agricultural Machinery based on GNSS Recordings: A graph convolutional neural network approach. Computers and Electronics in Agriculture, 198, 107082. \ud83d\udcd7 Liang, Y., Zhou, K. , & Wu, C. (2022). Environment scenario identification based on GNSS recordings for agricultural tractors. Computers and Electronics in Agriculture, 195, 106829.\ud83d\udea9[co-corresponding author] \ud83d\udcd7 Vahdanjoo, M., Zhou, K. , & S\u00f8rensen, C. A. G. (2020). Route planning for agricultural machines with multiple depots: manure application case study. Agronomy, 10(10), 1608.\ud83d\udea9[co-first author] \ud83d\udcd7 Nilsson, R. S., & Zhou, K . (2020). Method and bench-marking framework for coverage path planning in arable farming. Biosystems Engineering, 198, 248-265. \ud83d\udcd7 Nilsson, R. S., & Zhou, K . (2020). Decision support tool for operational planning of field operations. Agronomy, 10(2), 229. \ud83d\udcd7 Zhou, K. , Jensen, A. L., Bochtis, D., N\u00f8rremark, M., Kateris, D., & S\u00f8rensen, C. G. (2020). Metric map generation for autonomous field operations. Agronomy, 10(1), 83. \ud83d\udcd7 Zhou, K. , Bochtis, D., Jensen, A. L., Kateris, D., & S\u00f8rensen, C. G. (2020). Introduction of a new index of field operations efficiency. Applied Sciences, 10(1), 329. \ud83d\udcd7 Rodias, E., Berruto, R., Busato, P., Bochtis, D., S\u00f8rensen, C. G., & Zhou, K (2017). Energy savings from optimised in-field route planning for agricultural machinery. Sustainability, 9(11), 1956. \ud83d\udcd7 Zhou, K. , Jensen, A. L., Bochtis, D. D., & S\u00f8rensen, C. G. (2015). Quantifying the benefits of alternative fieldwork patterns in a potato cultivation system. Computers and Electronics in Agriculture, 119, 228-240. \ud83d\udcd7 Zhou, K. , Jensen, A. L., Bochtis, D. D., & S\u00f8rensen, C. G. (2015). Simulation model for the sequential in-field machinery operations in a potato production system. Computers and Electronics in Agriculture, 116, 173-186. \ud83d\udcd7 Bochtis, D., Griepentrog, H. W., Vougioukas, S., Busato, P., Berruto, R., & Zhou, K . (2015). Route planning for orchard operations. Computers and electronics in agriculture, 113, 51-60. \ud83d\udcd7 Zhou, K. , Jensen, A. L., Bochtis, D., & S\u00f8rensen, C. G. (2015). Performance of machinery in potato production in one growing season. Spanish journal of agricultural research, 13(4), 6. \ud83d\udcd7 Zhou, K. , Jensen, A. L., S\u00f8rensen, C. G., Busato, P., & Bothtis, D. D. (2014). Agricultural operations planning in fields with multiple obstacle areas. Computers and electronics in agriculture, 109, 12-22. Conference proceedings \u00b6 \ud83d\udcd7 Zhou, K. , Bochtis, D. (2015, September). Route Planning For Capacitated Agricultural Machines Based On Ant Colony Algorithms. In HAICTA (pp. 163-173). \ud83d\udcd7 Zhou, K. , Zhou, K., Bothtis, D. D., S\u00f8rensen, C. G., (2016). An object-oriented simulation tool for grain harvesting. CIGR-AgEng Conference, 26-29 June 2016, Aarhus \ud83d\udcd7 Jensen, A., la Riviere, I. J., de Bruin, S., Zhou, K. , J\u00f8rgensen, M. S., Pedersen, H., S\u00f8rensen, C. G. (2018). An online DSS for optimisation of traffic in fields with controlled traffic farming (CTF). In CIGR 2018: XIX. World Congress of CIGR (Commission Internationale du G\u00e9nie Rural) Program & Abstracts' Book: Sustainable Life for Children (pp. 154-154). CIGR. \ud83d\udcd7 Zhou, K. , Jensen, A. L., Bochtis, D., S\u00f8rensen, C. G. A Web\u2013based Tool for Comparing Field Area Coverage Practices. CIOSTA XXXV Conference: CIGR V Conference, Billund, Denmark; 07/2013. \ud83d\udcd7 Gunnarsson, E. Rodias, J. Kusk, Zhou, K. . M. A. F. Jensen, D. D. Bochtis. Biomass crop allocation problem. International Commission of Agricultural and Biological Engineers, Section V. CIOSTA XXXV Conference \u201cFrom Effective to Intelligent Agriculture and Forestry\u201d, Billund, Denmark, 3-5 July 2013. Patents \u00b6 \ud83d\udcd5 Zhou, K. , (2019). System And Method For Windrow Path Planning. U. S. Patent Application No. 202017135996. Washington, DC: U. S. Patent and Trademark Office. \ud83d\udcd5 Zhou, K. , System And Method For Windrow Path Planning. U. S. Patent Application No. 202017135990. Washington, DC: U. S. Patent and Trademark Office. \ud83d\udcd5 Zhou, K. , Nilsson, R. S., Kenneth, G. L. (2022). Operational path planning. G. B. Patent Application No. 202201115. Newport, South East Wales: Intellectual Property Office. \ud83d\udcd5 Zhou, K. , Nilsson, R. S., Kenneth, G. L. (2022). Operational path planning. G. B. Patent Application No. 202202445. Newport, South East Wales: Intellectual Property Office. \ud83d\udcd5 Zhou, K. , Nilsson, R. S., Kenneth, G. L. (2022). Wayline Generation for farming machine guidance. Patent Application No. Unkown yet. Washington, DC: U. S. Patent and Trademark Office.","title":"Publications"},{"location":"home/publications/#publications","text":"\ud83d\udcd7 : Published \ud83d\udcd5 : Under review","title":"Publications"},{"location":"home/publications/#journal-papers","text":"\ud83d\udcd7 Chen, Y., Li, G., Zhang, X., Jia, J., Zhou, K. , & Wu, C. (2022). Identifying field and road modes of agricultural Machinery based on GNSS Recordings: A graph convolutional neural network approach. Computers and Electronics in Agriculture, 198, 107082. \ud83d\udcd7 Liang, Y., Zhou, K. , & Wu, C. (2022). Environment scenario identification based on GNSS recordings for agricultural tractors. Computers and Electronics in Agriculture, 195, 106829.\ud83d\udea9[co-corresponding author] \ud83d\udcd7 Vahdanjoo, M., Zhou, K. , & S\u00f8rensen, C. A. G. (2020). Route planning for agricultural machines with multiple depots: manure application case study. Agronomy, 10(10), 1608.\ud83d\udea9[co-first author] \ud83d\udcd7 Nilsson, R. S., & Zhou, K . (2020). Method and bench-marking framework for coverage path planning in arable farming. Biosystems Engineering, 198, 248-265. \ud83d\udcd7 Nilsson, R. S., & Zhou, K . (2020). Decision support tool for operational planning of field operations. Agronomy, 10(2), 229. \ud83d\udcd7 Zhou, K. , Jensen, A. L., Bochtis, D., N\u00f8rremark, M., Kateris, D., & S\u00f8rensen, C. G. (2020). Metric map generation for autonomous field operations. Agronomy, 10(1), 83. \ud83d\udcd7 Zhou, K. , Bochtis, D., Jensen, A. L., Kateris, D., & S\u00f8rensen, C. G. (2020). Introduction of a new index of field operations efficiency. Applied Sciences, 10(1), 329. \ud83d\udcd7 Rodias, E., Berruto, R., Busato, P., Bochtis, D., S\u00f8rensen, C. G., & Zhou, K (2017). Energy savings from optimised in-field route planning for agricultural machinery. Sustainability, 9(11), 1956. \ud83d\udcd7 Zhou, K. , Jensen, A. L., Bochtis, D. D., & S\u00f8rensen, C. G. (2015). Quantifying the benefits of alternative fieldwork patterns in a potato cultivation system. Computers and Electronics in Agriculture, 119, 228-240. \ud83d\udcd7 Zhou, K. , Jensen, A. L., Bochtis, D. D., & S\u00f8rensen, C. G. (2015). Simulation model for the sequential in-field machinery operations in a potato production system. Computers and Electronics in Agriculture, 116, 173-186. \ud83d\udcd7 Bochtis, D., Griepentrog, H. W., Vougioukas, S., Busato, P., Berruto, R., & Zhou, K . (2015). Route planning for orchard operations. Computers and electronics in agriculture, 113, 51-60. \ud83d\udcd7 Zhou, K. , Jensen, A. L., Bochtis, D., & S\u00f8rensen, C. G. (2015). Performance of machinery in potato production in one growing season. Spanish journal of agricultural research, 13(4), 6. \ud83d\udcd7 Zhou, K. , Jensen, A. L., S\u00f8rensen, C. G., Busato, P., & Bothtis, D. D. (2014). Agricultural operations planning in fields with multiple obstacle areas. Computers and electronics in agriculture, 109, 12-22.","title":"Journal papers"},{"location":"home/publications/#conference-proceedings","text":"\ud83d\udcd7 Zhou, K. , Bochtis, D. (2015, September). Route Planning For Capacitated Agricultural Machines Based On Ant Colony Algorithms. In HAICTA (pp. 163-173). \ud83d\udcd7 Zhou, K. , Zhou, K., Bothtis, D. D., S\u00f8rensen, C. G., (2016). An object-oriented simulation tool for grain harvesting. CIGR-AgEng Conference, 26-29 June 2016, Aarhus \ud83d\udcd7 Jensen, A., la Riviere, I. J., de Bruin, S., Zhou, K. , J\u00f8rgensen, M. S., Pedersen, H., S\u00f8rensen, C. G. (2018). An online DSS for optimisation of traffic in fields with controlled traffic farming (CTF). In CIGR 2018: XIX. World Congress of CIGR (Commission Internationale du G\u00e9nie Rural) Program & Abstracts' Book: Sustainable Life for Children (pp. 154-154). CIGR. \ud83d\udcd7 Zhou, K. , Jensen, A. L., Bochtis, D., S\u00f8rensen, C. G. A Web\u2013based Tool for Comparing Field Area Coverage Practices. CIOSTA XXXV Conference: CIGR V Conference, Billund, Denmark; 07/2013. \ud83d\udcd7 Gunnarsson, E. Rodias, J. Kusk, Zhou, K. . M. A. F. Jensen, D. D. Bochtis. Biomass crop allocation problem. International Commission of Agricultural and Biological Engineers, Section V. CIOSTA XXXV Conference \u201cFrom Effective to Intelligent Agriculture and Forestry\u201d, Billund, Denmark, 3-5 July 2013.","title":"Conference proceedings"},{"location":"home/publications/#patents","text":"\ud83d\udcd5 Zhou, K. , (2019). System And Method For Windrow Path Planning. U. S. Patent Application No. 202017135996. Washington, DC: U. S. Patent and Trademark Office. \ud83d\udcd5 Zhou, K. , System And Method For Windrow Path Planning. U. S. Patent Application No. 202017135990. Washington, DC: U. S. Patent and Trademark Office. \ud83d\udcd5 Zhou, K. , Nilsson, R. S., Kenneth, G. L. (2022). Operational path planning. G. B. Patent Application No. 202201115. Newport, South East Wales: Intellectual Property Office. \ud83d\udcd5 Zhou, K. , Nilsson, R. S., Kenneth, G. L. (2022). Operational path planning. G. B. Patent Application No. 202202445. Newport, South East Wales: Intellectual Property Office. \ud83d\udcd5 Zhou, K. , Nilsson, R. S., Kenneth, G. L. (2022). Wayline Generation for farming machine guidance. Patent Application No. Unkown yet. Washington, DC: U. S. Patent and Trademark Office.","title":"Patents"},{"location":"home/certificate/certificate/","text":"Spring Framework \u00b6 Machine Learning \u00b6","title":"Course Certificate"},{"location":"home/certificate/certificate/#spring-framework","text":"","title":"Spring Framework"},{"location":"home/certificate/certificate/#machine-learning","text":"","title":"Machine Learning"},{"location":"home/data-handling/Pandas-split-columns/","text":"Split a text column into two columns in Pandas DataFrame \u00b6 Using Series.str.split() functions \u00b6 df = pd . DataFrame ({ 'name' : [ 'John Larter' , 'Robert Junior' , 'Jonny Depp' ], 'age' :[ 32 , 34 , 36 ]}) print ( \"Given Dataframe is : \\n \" , df ) # bydefault splitting is done on the basis of single space. print ( \" \\n Splitting 'Name' column into two different columns : \\n \" , df . name . str . split ( expand = True )) The output is Split Name column into \u201cFirst\u201d and \u201cLast\u201d column respectively and add it to the existing Dataframe. # create a new data frame df = pd . DataFrame ({ 'name' : [ 'John_Larter' , 'Robert_Junior' , 'Jonny_Depp' ], 'age' :[ 32 , 34 , 36 ]}) print ( \"Given Dataframe is : \\n \" , df ) # Adding two new columns to the existing dataframe. # bydefault splitting is done on the basis of '_'. df [[ 'First' , 'Last' ]] = df . name . str . split ( '-' , expand = True ) print ( \" \\n After adding two new columns : \\n \" , df ) The output is","title":"Split a text column into two columns in Pandas DataFrame"},{"location":"home/data-handling/Pandas-split-columns/#split-a-text-column-into-two-columns-in-pandas-dataframe","text":"","title":"Split a text column into two columns in Pandas DataFrame"},{"location":"home/data-handling/Pandas-split-columns/#using-seriesstrsplit-functions","text":"df = pd . DataFrame ({ 'name' : [ 'John Larter' , 'Robert Junior' , 'Jonny Depp' ], 'age' :[ 32 , 34 , 36 ]}) print ( \"Given Dataframe is : \\n \" , df ) # bydefault splitting is done on the basis of single space. print ( \" \\n Splitting 'Name' column into two different columns : \\n \" , df . name . str . split ( expand = True )) The output is Split Name column into \u201cFirst\u201d and \u201cLast\u201d column respectively and add it to the existing Dataframe. # create a new data frame df = pd . DataFrame ({ 'name' : [ 'John_Larter' , 'Robert_Junior' , 'Jonny_Depp' ], 'age' :[ 32 , 34 , 36 ]}) print ( \"Given Dataframe is : \\n \" , df ) # Adding two new columns to the existing dataframe. # bydefault splitting is done on the basis of '_'. df [[ 'First' , 'Last' ]] = df . name . str . split ( '-' , expand = True ) print ( \" \\n After adding two new columns : \\n \" , df ) The output is","title":"Using Series.str.split() functions"},{"location":"home/data-handling/Pandas%3Arename-columns/","text":"Rename columns in Pandas \u00b6 You can use one of the following three methods to rename columns in a pandas DataFrame: Method 1: Rename Specific Columns df . rename ( columns = { 'old_col1' : 'new_col1' , 'old_col2' : 'new_col2' }, inplace = True ) Code example: import pandas as pd #define DataFrame df = pd . DataFrame ({ 'team' :[ 'A' , 'A' , 'A' , 'A' , 'B' , 'B' , 'B' , 'B' ], 'points' : [ 25 , 12 , 15 , 14 , 19 , 23 , 25 , 29 ], 'assists' : [ 5 , 7 , 7 , 9 , 12 , 9 , 9 , 4 ], 'rebounds' : [ 11 , 8 , 10 , 6 , 6 , 5 , 9 , 12 ]}) #list column names list ( df ) [ 'team' , 'points' , 'assists' , 'rebounds' ] #rename specific column names df . rename ( columns = { 'team' : 'team_name' , 'points' : 'points_scored' }, inplace = True ) #view updated list of column names list ( df ) [ 'team_name' , 'points_scored' , 'assists' , 'rebounds' ] Method 2: Rename All Columns df . columns = [ 'new_col1' , 'new_col2' , 'new_col3' , 'new_col4' ] Code example: import pandas as pd #define DataFrame df = pd . DataFrame ({ 'team' :[ 'A' , 'A' , 'A' , 'A' , 'B' , 'B' , 'B' , 'B' ], 'points' : [ 25 , 12 , 15 , 14 , 19 , 23 , 25 , 29 ], 'assists' : [ 5 , 7 , 7 , 9 , 12 , 9 , 9 , 4 ], 'rebounds' : [ 11 , 8 , 10 , 6 , 6 , 5 , 9 , 12 ]}) #list column names list ( df ) [ 'team' , 'points' , 'assists' , 'rebounds' ] #rename all column names df . columns = [ '_team' , '_points' , '_assists' , '_rebounds' ] #view updated list of column names list ( df ) [ '_team' , '_points' , '_assists' , '_rebounds' ] Method 3: Replace Specific Characters in Columns df . columns = df . columns . str . replace ( 'old_char' , 'new_char' ) Code example: import pandas as pd #define DataFrame df = pd . DataFrame ({ '$team' :[ 'A' , 'A' , 'A' , 'A' , 'B' , 'B' , 'B' , 'B' ], '$points' : [ 25 , 12 , 15 , 14 , 19 , 23 , 25 , 29 ], '$assists' : [ 5 , 7 , 7 , 9 , 12 , 9 , 9 , 4 ], '$rebounds' : [ 11 , 8 , 10 , 6 , 6 , 5 , 9 , 12 ]}) #list column names list ( df ) [ 'team' , 'points' , 'assists' , 'rebounds' ] #rename $ with blank in every column name df . columns = df . columns . str . replace ( '$' , '' ) #view updated list of column names list ( df ) [ 'team' , 'points' , 'assists' , 'rebounds' ] Reference and related articles How to Rename Columns in Pandas","title":"Rename columns in Pandas"},{"location":"home/data-handling/Pandas%3Arename-columns/#rename-columns-in-pandas","text":"You can use one of the following three methods to rename columns in a pandas DataFrame: Method 1: Rename Specific Columns df . rename ( columns = { 'old_col1' : 'new_col1' , 'old_col2' : 'new_col2' }, inplace = True ) Code example: import pandas as pd #define DataFrame df = pd . DataFrame ({ 'team' :[ 'A' , 'A' , 'A' , 'A' , 'B' , 'B' , 'B' , 'B' ], 'points' : [ 25 , 12 , 15 , 14 , 19 , 23 , 25 , 29 ], 'assists' : [ 5 , 7 , 7 , 9 , 12 , 9 , 9 , 4 ], 'rebounds' : [ 11 , 8 , 10 , 6 , 6 , 5 , 9 , 12 ]}) #list column names list ( df ) [ 'team' , 'points' , 'assists' , 'rebounds' ] #rename specific column names df . rename ( columns = { 'team' : 'team_name' , 'points' : 'points_scored' }, inplace = True ) #view updated list of column names list ( df ) [ 'team_name' , 'points_scored' , 'assists' , 'rebounds' ] Method 2: Rename All Columns df . columns = [ 'new_col1' , 'new_col2' , 'new_col3' , 'new_col4' ] Code example: import pandas as pd #define DataFrame df = pd . DataFrame ({ 'team' :[ 'A' , 'A' , 'A' , 'A' , 'B' , 'B' , 'B' , 'B' ], 'points' : [ 25 , 12 , 15 , 14 , 19 , 23 , 25 , 29 ], 'assists' : [ 5 , 7 , 7 , 9 , 12 , 9 , 9 , 4 ], 'rebounds' : [ 11 , 8 , 10 , 6 , 6 , 5 , 9 , 12 ]}) #list column names list ( df ) [ 'team' , 'points' , 'assists' , 'rebounds' ] #rename all column names df . columns = [ '_team' , '_points' , '_assists' , '_rebounds' ] #view updated list of column names list ( df ) [ '_team' , '_points' , '_assists' , '_rebounds' ] Method 3: Replace Specific Characters in Columns df . columns = df . columns . str . replace ( 'old_char' , 'new_char' ) Code example: import pandas as pd #define DataFrame df = pd . DataFrame ({ '$team' :[ 'A' , 'A' , 'A' , 'A' , 'B' , 'B' , 'B' , 'B' ], '$points' : [ 25 , 12 , 15 , 14 , 19 , 23 , 25 , 29 ], '$assists' : [ 5 , 7 , 7 , 9 , 12 , 9 , 9 , 4 ], '$rebounds' : [ 11 , 8 , 10 , 6 , 6 , 5 , 9 , 12 ]}) #list column names list ( df ) [ 'team' , 'points' , 'assists' , 'rebounds' ] #rename $ with blank in every column name df . columns = df . columns . str . replace ( '$' , '' ) #view updated list of column names list ( df ) [ 'team' , 'points' , 'assists' , 'rebounds' ] Reference and related articles How to Rename Columns in Pandas","title":"Rename columns in Pandas"},{"location":"home/machine-learning/feature-scaling/","text":"Feature scaling \u00b6 Feature scaling in machine learning is one of the most critical steps during the pre-processing of data before creating a machine learning model, which is used to normalize the range of independent variables or features of data. Methods for Scaling \u00b6 Min-max normalization \u00b6 Also known as min-max scaling or min-max normalization , rescaling is the simplest method and consists in rescaling the range of features to scale the range in [0, 1] or [\u22121, 1]. Selecting the target range depends on the nature of the data. The general formula for a min-max of [0, 1] is given as: \\[ x'={\\frac{x-{\\text{min}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}} \\] where \\(x\\) is an original value, \\(x'\\) is the normalized value. For example, suppose that we have the students' weight data, and the students' weights span [160 pounds, 200 pounds]. To rescale this data, we first subtract 160 from each student's weight and divide the result by the difference between the maximum and minimum weights, 40. To rescale a range between an arbitrary set of values \\([a, b]\\) , the formula becomes: \\[ {\\displaystyle x'=a+{\\frac {(x-{\\text{min}}(x))(b-a)}{{\\text{max}}(x)-{\\text{min}}(x)}}} \\] where \\(a\\) , \\(b\\) are the min-max values. Mean normalization \u00b6 \\[ x' = \\frac{x- \\text{average}(x)}{\\text{max}(x)-\\text{min}(x)} \\] where \\(x\\) is an original value, \\(x'\\) is the normalized value. Standardization (Z-score Normalization) \u00b6 Feature standardization makes the values of each feature in the data have zero-mean . (when subtracting the mean in the numerator) and unit-variance . This method is widely used for normalization in many machine learning algorithms (e.g., support vector machines, logistic regression, and artificial neural networks). The general method of calculation is to determine the distribution mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation. \\[ x' = \\frac{x- \\bar{x}}{\\sigma} \\] where \\(x\\) is the original feature vector, \\({\\bar {x}}={\\text{average}}(x)\\) is the mean of that feature vector, and \\(\\sigma\\) is its standard deviation. When to use Normalization or Standardizatio \u00b6 If you have ever built a machine learning pipeline, you must have always faced this question of whether to Normalize or to Standardize. While there is no obvious answer to this question, it really depends on the application, there are still a few generalizations that can be drawn. Normalization is good to use when the distribution of data does not follow a Gaussian distribution . It can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors. In Neural Networks algorithm that require data on a 0\u20131 scale, normalization is an essential pre-processing step. Another popular example of data normalization is image processing, where pixel intensities have to be normalized to fit within a certain range (i.e., 0 to 255 for the RGB color range). Standardization can be helpful in cases where the data follows a Gaussian distribution . Though this does not have to be necessarily true. Since standardization does not have a bounding range, so, even if there are outliers in the data, they will not be affected by standardization. In clustering analyses, standardization comes in handy to compare similarities between features based on certain distance measures. Another prominent example is the Principal Component Analysis, where we usually prefer standardization over Min-Max scaling since we are interested in the components that maximize the variance. There are some points which can be considered while deciding whether we need Standardization or Normalization Standardization may be used when data represent Gaussian Distribution, while Normalization is great with Non-Gaussian Distribution Impact of Outliers is very high in Normalization To conclude, you can always start by fitting your model to raw, normalized, and standardized data and compare the performance for the best results. Reference and related articles All about Feature Scaling When to perform a Feature Scaling? Wiki-Feature scaling","title":"Feature scaling"},{"location":"home/machine-learning/feature-scaling/#feature-scaling","text":"Feature scaling in machine learning is one of the most critical steps during the pre-processing of data before creating a machine learning model, which is used to normalize the range of independent variables or features of data.","title":"Feature scaling"},{"location":"home/machine-learning/feature-scaling/#methods-for-scaling","text":"","title":"Methods for Scaling"},{"location":"home/machine-learning/feature-scaling/#min-max-normalization","text":"Also known as min-max scaling or min-max normalization , rescaling is the simplest method and consists in rescaling the range of features to scale the range in [0, 1] or [\u22121, 1]. Selecting the target range depends on the nature of the data. The general formula for a min-max of [0, 1] is given as: \\[ x'={\\frac{x-{\\text{min}}(x)}{{\\text{max}}(x)-{\\text{min}}(x)}} \\] where \\(x\\) is an original value, \\(x'\\) is the normalized value. For example, suppose that we have the students' weight data, and the students' weights span [160 pounds, 200 pounds]. To rescale this data, we first subtract 160 from each student's weight and divide the result by the difference between the maximum and minimum weights, 40. To rescale a range between an arbitrary set of values \\([a, b]\\) , the formula becomes: \\[ {\\displaystyle x'=a+{\\frac {(x-{\\text{min}}(x))(b-a)}{{\\text{max}}(x)-{\\text{min}}(x)}}} \\] where \\(a\\) , \\(b\\) are the min-max values.","title":"Min-max normalization"},{"location":"home/machine-learning/feature-scaling/#mean-normalization","text":"\\[ x' = \\frac{x- \\text{average}(x)}{\\text{max}(x)-\\text{min}(x)} \\] where \\(x\\) is an original value, \\(x'\\) is the normalized value.","title":"Mean normalization"},{"location":"home/machine-learning/feature-scaling/#standardization-z-score-normalization","text":"Feature standardization makes the values of each feature in the data have zero-mean . (when subtracting the mean in the numerator) and unit-variance . This method is widely used for normalization in many machine learning algorithms (e.g., support vector machines, logistic regression, and artificial neural networks). The general method of calculation is to determine the distribution mean and standard deviation for each feature. Next we subtract the mean from each feature. Then we divide the values (mean is already subtracted) of each feature by its standard deviation. \\[ x' = \\frac{x- \\bar{x}}{\\sigma} \\] where \\(x\\) is the original feature vector, \\({\\bar {x}}={\\text{average}}(x)\\) is the mean of that feature vector, and \\(\\sigma\\) is its standard deviation.","title":"Standardization (Z-score Normalization)"},{"location":"home/machine-learning/feature-scaling/#when-to-use-normalization-or-standardizatio","text":"If you have ever built a machine learning pipeline, you must have always faced this question of whether to Normalize or to Standardize. While there is no obvious answer to this question, it really depends on the application, there are still a few generalizations that can be drawn. Normalization is good to use when the distribution of data does not follow a Gaussian distribution . It can be useful in algorithms that do not assume any distribution of the data like K-Nearest Neighbors. In Neural Networks algorithm that require data on a 0\u20131 scale, normalization is an essential pre-processing step. Another popular example of data normalization is image processing, where pixel intensities have to be normalized to fit within a certain range (i.e., 0 to 255 for the RGB color range). Standardization can be helpful in cases where the data follows a Gaussian distribution . Though this does not have to be necessarily true. Since standardization does not have a bounding range, so, even if there are outliers in the data, they will not be affected by standardization. In clustering analyses, standardization comes in handy to compare similarities between features based on certain distance measures. Another prominent example is the Principal Component Analysis, where we usually prefer standardization over Min-Max scaling since we are interested in the components that maximize the variance. There are some points which can be considered while deciding whether we need Standardization or Normalization Standardization may be used when data represent Gaussian Distribution, while Normalization is great with Non-Gaussian Distribution Impact of Outliers is very high in Normalization To conclude, you can always start by fitting your model to raw, normalized, and standardized data and compare the performance for the best results. Reference and related articles All about Feature Scaling When to perform a Feature Scaling? Wiki-Feature scaling","title":"When to use Normalization or Standardizatio"},{"location":"home/machine-learning/gradient-descent/","text":"Gradient Descent for Linear Regression \u00b6 Gradient descent summary \u00b6 A linear model that predicts \\(f_{w, b}(x^{(i)})\\) : \\[f_{w, b}(x^{(i)}) = wx^{(i)} + b \\tag{1}\\] In linear regression, you utilize input training data to fit the parameters \\(w\\) , \\(b\\) by minimizing a measure of the error between our predictions \\(f_{w, b}(x^{(i)})\\) and the actual data \\(y^{(i)}\\) . The measure is called the \\(cost\\) , \\(J(w, b)\\) . In training, you measure the cost over all of our training samples \\((x^{(i)}, y^{(i)})\\) \\[J(w, b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w, b}(x^{(i)}) - y^{(i)})^2\\tag{2}\\] gradient descent was described as: \\[\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline \\; w &= w - \\alpha \\frac{\\partial J(w, b)}{\\partial w} \\tag{3} \\; \\newline b &= b - \\alpha \\frac{\\partial J(w, b)}{\\partial b} \\newline \\rbrace \\end{align*}\\] where, parameters \\(w\\) , \\(b\\) are updated simultaneously. The gradient is defined as: \\[ \\begin{align} \\frac{\\partial J(w, b)}{\\partial w} &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w, b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\ \\frac{\\partial J(w, b)}{\\partial b} &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w, b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\ \\end{align} \\] Here simultaniously means that you calculate the partial derivatives for all the parameters before updating any of the parameters. compute_gradient \u00b6 compute_gradient implements (4) and (5) above and returns \\(\\frac{\\partial J(w, b)}{\\partial w}\\) , \\(\\frac{\\partial J(w, b)}{\\partial b}\\) . The embedded comments describe the operations. def compute_gradient ( x , y , w , b ): \"\"\" Computes the gradient for linear regression Args: x (ndarray (m,)): Data, m examples y (ndarray (m,)): target values w,b (scalar) : model parameters Returns dj_dw (scalar): The gradient of the cost w.r.t. the parameters w dj_db (scalar): The gradient of the cost w.r.t. the parameter b \"\"\" # Number of training examples m = x . shape [ 0 ] dj_dw = 0 dj_db = 0 for i in range ( m ): f_wb = w * x [ i ] + b dj_dw_i = ( f_wb - y [ i ]) * x [ i ] dj_db_i = f_wb - y [ i ] dj_db += dj_db_i dj_dw += dj_dw_i dj_dw = dj_dw / m dj_db = dj_db / m return dj_dw , dj_db Above, the left plot shows \\(\\frac{\\partial J(w, b)}{\\partial w}\\) or the slope of the cost curve relative to \\(w\\) at three points. On the right side of the plot, the derivative is positive, while on the left it is negative. Due to the 'bowl shape', the derivatives will always lead gradient descent toward the bottom where the gradient is zero. The left plot has fixed \\(b=100\\) . Gradient descent will utilize both \\(\\frac{\\partial J(w, b)}{\\partial w}\\) and \\(\\frac{\\partial J(w, b)}{\\partial b}\\) to update parameters. The 'quiver plot' on the right provides a means of viewing the gradient of both parameters. The arrow sizes reflect the magnitude of the gradient at that point. The direction and slope of the arrow reflects the ratio of \\(\\frac{\\partial J(w, b)}{\\partial w}\\) and \\(\\frac{\\partial J(w, b)}{\\partial b}\\) at that point. Note that the gradient points away from the minimum. Review equation (3) above. The scaled gradient is subtracted from the current value of \\(w\\) or \\(b\\) . This moves the parameter in a direction that will reduce cost. Cost versus iterations of gradient descent \u00b6 A plot of cost versus iterations is a useful measure of progress in gradient descent. Cost should always decrease in successful runs. The change in cost is so rapid initially, it is useful to plot the initial decent on a different scale than the final descent. In the plots below, note the scale of cost on the axes and the iteration step. Contour plot of const J(w, b) versus b, w with path of gradient descent \u00b6 the contour plot shows the \\(cost(w, b)\\) over a range of \\(w\\) and \\(b\\) . Cost levels are represented by the rings. Overlayed, using red arrows, is the path of gradient descent. Here are some things to note: The path makes steady (monotonic) progress toward its goal. initial steps are much larger than the steps near the goal. Zooming in , we can see that final steps of gradient descent. Note the distance between steps shrinks as the gradient approaches zero. Reference Coursera:Supervised Machine Learning","title":"Gradient Descent for Linear Regression"},{"location":"home/machine-learning/gradient-descent/#gradient-descent-for-linear-regression","text":"","title":"Gradient Descent for Linear Regression"},{"location":"home/machine-learning/gradient-descent/#gradient-descent-summary","text":"A linear model that predicts \\(f_{w, b}(x^{(i)})\\) : \\[f_{w, b}(x^{(i)}) = wx^{(i)} + b \\tag{1}\\] In linear regression, you utilize input training data to fit the parameters \\(w\\) , \\(b\\) by minimizing a measure of the error between our predictions \\(f_{w, b}(x^{(i)})\\) and the actual data \\(y^{(i)}\\) . The measure is called the \\(cost\\) , \\(J(w, b)\\) . In training, you measure the cost over all of our training samples \\((x^{(i)}, y^{(i)})\\) \\[J(w, b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w, b}(x^{(i)}) - y^{(i)})^2\\tag{2}\\] gradient descent was described as: \\[\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline \\; w &= w - \\alpha \\frac{\\partial J(w, b)}{\\partial w} \\tag{3} \\; \\newline b &= b - \\alpha \\frac{\\partial J(w, b)}{\\partial b} \\newline \\rbrace \\end{align*}\\] where, parameters \\(w\\) , \\(b\\) are updated simultaneously. The gradient is defined as: \\[ \\begin{align} \\frac{\\partial J(w, b)}{\\partial w} &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w, b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\ \\frac{\\partial J(w, b)}{\\partial b} &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w, b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\ \\end{align} \\] Here simultaniously means that you calculate the partial derivatives for all the parameters before updating any of the parameters.","title":"Gradient descent summary"},{"location":"home/machine-learning/gradient-descent/#compute_gradient","text":"compute_gradient implements (4) and (5) above and returns \\(\\frac{\\partial J(w, b)}{\\partial w}\\) , \\(\\frac{\\partial J(w, b)}{\\partial b}\\) . The embedded comments describe the operations. def compute_gradient ( x , y , w , b ): \"\"\" Computes the gradient for linear regression Args: x (ndarray (m,)): Data, m examples y (ndarray (m,)): target values w,b (scalar) : model parameters Returns dj_dw (scalar): The gradient of the cost w.r.t. the parameters w dj_db (scalar): The gradient of the cost w.r.t. the parameter b \"\"\" # Number of training examples m = x . shape [ 0 ] dj_dw = 0 dj_db = 0 for i in range ( m ): f_wb = w * x [ i ] + b dj_dw_i = ( f_wb - y [ i ]) * x [ i ] dj_db_i = f_wb - y [ i ] dj_db += dj_db_i dj_dw += dj_dw_i dj_dw = dj_dw / m dj_db = dj_db / m return dj_dw , dj_db Above, the left plot shows \\(\\frac{\\partial J(w, b)}{\\partial w}\\) or the slope of the cost curve relative to \\(w\\) at three points. On the right side of the plot, the derivative is positive, while on the left it is negative. Due to the 'bowl shape', the derivatives will always lead gradient descent toward the bottom where the gradient is zero. The left plot has fixed \\(b=100\\) . Gradient descent will utilize both \\(\\frac{\\partial J(w, b)}{\\partial w}\\) and \\(\\frac{\\partial J(w, b)}{\\partial b}\\) to update parameters. The 'quiver plot' on the right provides a means of viewing the gradient of both parameters. The arrow sizes reflect the magnitude of the gradient at that point. The direction and slope of the arrow reflects the ratio of \\(\\frac{\\partial J(w, b)}{\\partial w}\\) and \\(\\frac{\\partial J(w, b)}{\\partial b}\\) at that point. Note that the gradient points away from the minimum. Review equation (3) above. The scaled gradient is subtracted from the current value of \\(w\\) or \\(b\\) . This moves the parameter in a direction that will reduce cost.","title":"compute_gradient"},{"location":"home/machine-learning/gradient-descent/#cost-versus-iterations-of-gradient-descent","text":"A plot of cost versus iterations is a useful measure of progress in gradient descent. Cost should always decrease in successful runs. The change in cost is so rapid initially, it is useful to plot the initial decent on a different scale than the final descent. In the plots below, note the scale of cost on the axes and the iteration step.","title":"Cost versus iterations of gradient descent"},{"location":"home/machine-learning/gradient-descent/#contour-plot-of-const-jw-b-versus-b-w-with-path-of-gradient-descent","text":"the contour plot shows the \\(cost(w, b)\\) over a range of \\(w\\) and \\(b\\) . Cost levels are represented by the rings. Overlayed, using red arrows, is the path of gradient descent. Here are some things to note: The path makes steady (monotonic) progress toward its goal. initial steps are much larger than the steps near the goal. Zooming in , we can see that final steps of gradient descent. Note the distance between steps shrinks as the gradient approaches zero. Reference Coursera:Supervised Machine Learning","title":"Contour plot of const J(w, b) versus b, w with path of gradient descent"},{"location":"home/machine-learning/learning_rate/","text":"Learning rate \u00b6 In machine learning and statistics, the learning rate \\(\\alpha\\) is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. Let's run gradient descent and try a few settings of \\(\\alpha\\) on our data set. The plot on the right shows the value of one of the parameters, \\(w_0\\) . At each iteration, it is overshooting the optimal value and as a result, cost ends up increasing rather than approaching the minimum. Note that this is not a completely accurate picture as there are 4 parameters being modified each pass rather than just one. This plot is only showing \\(w_0\\) with the other parameters fixed at begin values. In this and later plots you may notice the blue and orange lines being slightly off. Let's try a bit smaller value and see what happens. On the left, you see that cost is decreasing as it should. On the right, you can see that \\(w_0\\) is still oscillating around the minimum, but it is decreasing each iteration rather than increasing. Note above that dj_dw[0] changes sign with each iteration as w[0] jumps over the optimal value. This alpha value will converge. You can vary the number of iterations to see how it behaves. Let's try a bit smaller value for \\(\\alpha\\) and see what happens. On the left, you see that cost is decreasing as it should. On the right you can see that \\(w_0\\) is decreasing without crossing the minimum. Note above that dj_w0 is negative throughout the run. This solution will also converge, though not quite as quickly as the previous example.","title":"Learning rate"},{"location":"home/machine-learning/learning_rate/#learning-rate","text":"In machine learning and statistics, the learning rate \\(\\alpha\\) is a tuning parameter in an optimization algorithm that determines the step size at each iteration while moving toward a minimum of a loss function. Let's run gradient descent and try a few settings of \\(\\alpha\\) on our data set. The plot on the right shows the value of one of the parameters, \\(w_0\\) . At each iteration, it is overshooting the optimal value and as a result, cost ends up increasing rather than approaching the minimum. Note that this is not a completely accurate picture as there are 4 parameters being modified each pass rather than just one. This plot is only showing \\(w_0\\) with the other parameters fixed at begin values. In this and later plots you may notice the blue and orange lines being slightly off. Let's try a bit smaller value and see what happens. On the left, you see that cost is decreasing as it should. On the right, you can see that \\(w_0\\) is still oscillating around the minimum, but it is decreasing each iteration rather than increasing. Note above that dj_dw[0] changes sign with each iteration as w[0] jumps over the optimal value. This alpha value will converge. You can vary the number of iterations to see how it behaves. Let's try a bit smaller value for \\(\\alpha\\) and see what happens. On the left, you see that cost is decreasing as it should. On the right you can see that \\(w_0\\) is decreasing without crossing the minimum. Note above that dj_w0 is negative throughout the run. This solution will also converge, though not quite as quickly as the previous example.","title":"Learning rate"},{"location":"home/statistics/outliers/","text":"Ways to identify outliers \u00b6 Outliers are extreme values that differ from most other data points in a dataset. They can have a big impact on your statistical analyses and skew the results of any hypothesis tests. True outliers should always be retained in your dataset because these just represent natural variations in your sample. Sorting method \u00b6 You can sort quantitative variables from low to high in ascending order and find out extremely low or extremely high values. Flag any extreme values that you find. This is a simple way to check whether you need to investigate certain data points before using more sophisticated methods. Data visualizations \u00b6 Two of the most common graphical ways of detecting outliers are the boxplot and the scatterplot. A boxplot is my favorite way. You can see here that the blue circles are outliers, where the open circles representing mild outliers, and closed circles representing extreme outliers: Statistical outlier detection \u00b6 Statistical outlier detection involves applying statistical tests or procedures to identify extreme values. You can convert extreme data points into z scores that tell you how many standard deviations away they are from the mean. If a value has a high enough or low enough z score, it can be considered an outlier. As a rule of thumb, values with a z score greater than 3 or less than \u20133 are often determined to be outliers 1 . Using the interquartile range(IQR) \u00b6 We can use the IQR method of identifying outliers to set up a \u201cfence\u201d outside of Q1 and Q3. Any values that fall outside of this fence are considered outliers. Interquartile range method: Sort your data from low to high Identify the first quartile (Q1), the median, and the third quartile (Q3). Calculate your IQR = Q3 \u2013 Q1 Calculate your upper fence = Q3 + (1.5 * IQR) Calculate your lower fence = Q1 \u2013 (1.5 * IQR) Use your fences to highlight any outliers, all values that fall outside your fences. Your outliers are any values greater than your upper fence or less than your lower fence. Reference How to Find Outliers https://cxl.com/blog/outliers/","title":"Ways to identify outliers"},{"location":"home/statistics/outliers/#ways-to-identify-outliers","text":"Outliers are extreme values that differ from most other data points in a dataset. They can have a big impact on your statistical analyses and skew the results of any hypothesis tests. True outliers should always be retained in your dataset because these just represent natural variations in your sample.","title":"Ways to identify outliers"},{"location":"home/statistics/outliers/#sorting-method","text":"You can sort quantitative variables from low to high in ascending order and find out extremely low or extremely high values. Flag any extreme values that you find. This is a simple way to check whether you need to investigate certain data points before using more sophisticated methods.","title":"Sorting method"},{"location":"home/statistics/outliers/#data-visualizations","text":"Two of the most common graphical ways of detecting outliers are the boxplot and the scatterplot. A boxplot is my favorite way. You can see here that the blue circles are outliers, where the open circles representing mild outliers, and closed circles representing extreme outliers:","title":"Data visualizations"},{"location":"home/statistics/outliers/#statistical-outlier-detection","text":"Statistical outlier detection involves applying statistical tests or procedures to identify extreme values. You can convert extreme data points into z scores that tell you how many standard deviations away they are from the mean. If a value has a high enough or low enough z score, it can be considered an outlier. As a rule of thumb, values with a z score greater than 3 or less than \u20133 are often determined to be outliers 1 .","title":"Statistical outlier detection"},{"location":"home/statistics/outliers/#using-the-interquartile-rangeiqr","text":"We can use the IQR method of identifying outliers to set up a \u201cfence\u201d outside of Q1 and Q3. Any values that fall outside of this fence are considered outliers. Interquartile range method: Sort your data from low to high Identify the first quartile (Q1), the median, and the third quartile (Q3). Calculate your IQR = Q3 \u2013 Q1 Calculate your upper fence = Q3 + (1.5 * IQR) Calculate your lower fence = Q1 \u2013 (1.5 * IQR) Use your fences to highlight any outliers, all values that fall outside your fences. Your outliers are any values greater than your upper fence or less than your lower fence. Reference How to Find Outliers https://cxl.com/blog/outliers/","title":"Using the interquartile range(IQR)"},{"location":"home/streamlit/display_text/","text":"Display texts \u00b6 import streamlit as st # Working with and Displaying Text st . text ( \"Hello World this is a text\" ) name = \"Johnny\" st . text ( \"This is so {} \" . format ( name )) # Title st . title ( \"This is a title\" ) # Header st . header ( \"This is a header\" ) # Subheader st . subheader ( \"This is a subheader\" ) # Markdown st . markdown ( \"## This is markdown\" ) # LaTeX st . latex ( r ''' a+a r^1+a r^2+a r^3 ''' ) st . latex ( r ''' a + ar + a r^2 + a r^3 + \\cdots + a r^{n-1} = \\sum_{k=0}^{n-1} ar^k = a \\left(\\frac{1-r^ {n} }{1-r}\\right) ''' ) # Caption used for captions, asides, footnotes, sidenotes, and other explanatory text. st . caption ( \"this is the caption\" ) # Code st . code ( \"x=2021\" ) code = '''def hello(): print(\"Hello, Streamlit!\")''' st . code ( code , language = 'python' ) # Displaying Colored Text/Bootstraps Alert st . success ( \"Successful\" ) st . warning ( \"This is danger\" ) st . info ( \"This is information\" ) st . error ( \"This is an error\" ) st . exception ( \"This is an exception\" ) # Superfunction st . write ( \"Normal Text\" ) st . write ( \"## This is a markdown text\" ) st . write ( 1 + 2 ) st . write ( dir ( st )) # Help Info st . help ( range ) Reference and related articles Text elements Python Tutorial: Streamlit","title":"Display texts"},{"location":"home/streamlit/display_text/#display-texts","text":"import streamlit as st # Working with and Displaying Text st . text ( \"Hello World this is a text\" ) name = \"Johnny\" st . text ( \"This is so {} \" . format ( name )) # Title st . title ( \"This is a title\" ) # Header st . header ( \"This is a header\" ) # Subheader st . subheader ( \"This is a subheader\" ) # Markdown st . markdown ( \"## This is markdown\" ) # LaTeX st . latex ( r ''' a+a r^1+a r^2+a r^3 ''' ) st . latex ( r ''' a + ar + a r^2 + a r^3 + \\cdots + a r^{n-1} = \\sum_{k=0}^{n-1} ar^k = a \\left(\\frac{1-r^ {n} }{1-r}\\right) ''' ) # Caption used for captions, asides, footnotes, sidenotes, and other explanatory text. st . caption ( \"this is the caption\" ) # Code st . code ( \"x=2021\" ) code = '''def hello(): print(\"Hello, Streamlit!\")''' st . code ( code , language = 'python' ) # Displaying Colored Text/Bootstraps Alert st . success ( \"Successful\" ) st . warning ( \"This is danger\" ) st . info ( \"This is information\" ) st . error ( \"This is an error\" ) st . exception ( \"This is an exception\" ) # Superfunction st . write ( \"Normal Text\" ) st . write ( \"## This is a markdown text\" ) st . write ( 1 + 2 ) st . write ( dir ( st )) # Help Info st . help ( range ) Reference and related articles Text elements Python Tutorial: Streamlit","title":"Display texts"},{"location":"home/streamlit/text_input/","text":"Text Inputs \u00b6 Single-line text input widget \u00b6 fname = st . text_input ( \"Enter Firstname\" ) Text Input Hide Password \u00b6 password = st . text_input ( \"Enter Password\" , type = 'password' ) st . title ( fname ) A multi-line text input widget \u00b6 message = st . text_area ( \"Enter Message\" , height = 100 ) st . write ( message ) Numbers \u00b6 number = st . number_input ( \"Enter Number\" , 1.0 , 25.0 ) Date Input \u00b6 myappointment = st . date_input ( \"Appointment\" ) Time Input \u00b6 mytime = st . time_input ( \"My Time\" ) Color Picker \u00b6 color = st . color_picker ( \"Select Color\" )","title":"Text Inputs"},{"location":"home/streamlit/text_input/#text-inputs","text":"","title":"Text Inputs"},{"location":"home/streamlit/text_input/#single-line-text-input-widget","text":"fname = st . text_input ( \"Enter Firstname\" )","title":"Single-line text input widget"},{"location":"home/streamlit/text_input/#text-input-hide-password","text":"password = st . text_input ( \"Enter Password\" , type = 'password' ) st . title ( fname )","title":"Text Input Hide Password"},{"location":"home/streamlit/text_input/#a-multi-line-text-input-widget","text":"message = st . text_area ( \"Enter Message\" , height = 100 ) st . write ( message )","title":"A multi-line text input widget"},{"location":"home/streamlit/text_input/#numbers","text":"number = st . number_input ( \"Enter Number\" , 1.0 , 25.0 )","title":"Numbers"},{"location":"home/streamlit/text_input/#date-input","text":"myappointment = st . date_input ( \"Appointment\" )","title":"Date Input"},{"location":"home/streamlit/text_input/#time-input","text":"mytime = st . time_input ( \"My Time\" )","title":"Time Input"},{"location":"home/streamlit/text_input/#color-picker","text":"color = st . color_picker ( \"Select Color\" )","title":"Color Picker"},{"location":"home/streamlit/what_is_streamlit/","text":"What is Streamlit? \u00b6 Streamlit is a free and open-source framework to rapidly build and share beautiful machine learning and data science web apps. It is a Python-based library specifically designed for machine learning engineers. Data scientists or machine learning engineers are not web developers and they're not interested in spending weeks learning to use these frameworks to build web apps. Instead, they want a tool that is easier to learn and to use, as long as it can display data and collect needed parameters for modeling. Streamlit allows you to create a stunning-looking application with only a few lines of code. Streamlit is the easiest way especially for people with no front-end knowledge to put their code into a web application: No front-end (html, js, css) experience or knowledge is required. You don't need to spend days or months to create a web app, you can create a really beautiful machine learning or data science app in only a few hours or even minutes. It is compatible with the majority of Python libraries (e.g. pandas, matplotlib, seaborn, plotly, Keras, PyTorch, SymPy(latex)). Less code is needed to create amazing web apps. Data caching simplifies and speeds up computation pipelines.","title":"What is Streamlit?"},{"location":"home/streamlit/what_is_streamlit/#what-is-streamlit","text":"Streamlit is a free and open-source framework to rapidly build and share beautiful machine learning and data science web apps. It is a Python-based library specifically designed for machine learning engineers. Data scientists or machine learning engineers are not web developers and they're not interested in spending weeks learning to use these frameworks to build web apps. Instead, they want a tool that is easier to learn and to use, as long as it can display data and collect needed parameters for modeling. Streamlit allows you to create a stunning-looking application with only a few lines of code. Streamlit is the easiest way especially for people with no front-end knowledge to put their code into a web application: No front-end (html, js, css) experience or knowledge is required. You don't need to spend days or months to create a web app, you can create a really beautiful machine learning or data science app in only a few hours or even minutes. It is compatible with the majority of Python libraries (e.g. pandas, matplotlib, seaborn, plotly, Keras, PyTorch, SymPy(latex)). Less code is needed to create amazing web apps. Data caching simplifies and speeds up computation pipelines.","title":"What is Streamlit?"},{"location":"home/streamlit/widgets/","text":"Streamlit Widget \u00b6 Working with Buttons \u00b6 name = \"Jesse\" if st . button ( \"Submit\" ): st . write ( \"Name: {} \" . format ( name . upper ())) If you have multiple buttons, then you can use \"key\" attribute to give a unique name for each button if st . button ( \"Submit\" , key = 'new02' ): st . write ( \"First Name: {} \" . format ( name . lower ())) To invoke a function when click on a button if 'count' not in st . session_state : st . session_state . count = 0 def increment_counter (): st . session_state . count += 1 st . button ( 'Increment' , on_click = increment_counter ) st . write ( 'You clicked to increment count, total count = ' , st . session_state . count ) Working with RadioButtons \u00b6 status = st . radio ( \"What is your status\" , ( \"Active\" , \"Inactive\" )) if status == 'Active' : st . success ( \"You are active\" ) elif status == \"Inactive\" : st . warning ( \"Inactive\" ) city_options = { 5 : \"Arizona - Chandler - 5\" , 4 : \"Arizona - Phoenix - 4\" , 3 : \"New Jersey - Newark -3\" , 2 : \"Oregon - Portland - 2\" , 1 : \"Seattle - Washington - 1\" , } city_mode = st . sidebar . radio ( label = \"Choose a city option:\" , options = ( 5 , 4 , 3 , 2 , 1 ), format_func = lambda x : city_options . get ( x ), ) st . write ( f 'You have chosen { city_options . get ( city_mode ) } ' f ' with the value { city_mode } .' ) Working with Checkbox \u00b6 if st . checkbox ( \"Show/Hide\" ): st . text ( \"Showing something\" ) Working with Expander \u00b6 if st . expander ( \"Python\" ): st . success ( \"Hello Python\" ) with st . expander ( \"Julia\" ): st . text ( \"hello Julia\" ) Working with one select \u00b6 my_lang = [ \"Python\" , \"Julia\" , \"Go\" , \"Rust\" ] choice = st . selectbox ( \"Language\" , my_lang ) st . write ( \"You selected {} \" . format ( choice )) Working with multiple select \u00b6 spoken_lang = ( \"English\" , \"French\" , \"Spanish\" , \"Twi\" ) my_spoken_lang = st . multiselect ( \"Spoken Lang\" , spoken_lang , default = \"English\" ) Working with slider \u00b6 The difference between st.slider and st.select_slider is that slider only accepts numerical or date/time data and takes a range as input, while select_slider accepts any datatype and takes an iterable set of options. Numbers (Int/Float/Dates) age = st . slider ( \"Age\" , 1 , 100 ) st . write ( \"age:\" , age ) Any Datatype using Select Slider color = st . select_slider ( \"Choose Color\" , options = [ \"yellow\" , \"red\" , \"blue\" , \"black\" , \"white\" ], value = ( \"yellow\" , \"red\" )) st . write ( \"color:\" , color )","title":"Streamlit Widget"},{"location":"home/streamlit/widgets/#streamlit-widget","text":"","title":"Streamlit Widget"},{"location":"home/streamlit/widgets/#working-with-buttons","text":"name = \"Jesse\" if st . button ( \"Submit\" ): st . write ( \"Name: {} \" . format ( name . upper ())) If you have multiple buttons, then you can use \"key\" attribute to give a unique name for each button if st . button ( \"Submit\" , key = 'new02' ): st . write ( \"First Name: {} \" . format ( name . lower ())) To invoke a function when click on a button if 'count' not in st . session_state : st . session_state . count = 0 def increment_counter (): st . session_state . count += 1 st . button ( 'Increment' , on_click = increment_counter ) st . write ( 'You clicked to increment count, total count = ' , st . session_state . count )","title":"Working with Buttons"},{"location":"home/streamlit/widgets/#working-with-radiobuttons","text":"status = st . radio ( \"What is your status\" , ( \"Active\" , \"Inactive\" )) if status == 'Active' : st . success ( \"You are active\" ) elif status == \"Inactive\" : st . warning ( \"Inactive\" ) city_options = { 5 : \"Arizona - Chandler - 5\" , 4 : \"Arizona - Phoenix - 4\" , 3 : \"New Jersey - Newark -3\" , 2 : \"Oregon - Portland - 2\" , 1 : \"Seattle - Washington - 1\" , } city_mode = st . sidebar . radio ( label = \"Choose a city option:\" , options = ( 5 , 4 , 3 , 2 , 1 ), format_func = lambda x : city_options . get ( x ), ) st . write ( f 'You have chosen { city_options . get ( city_mode ) } ' f ' with the value { city_mode } .' )","title":"Working with RadioButtons"},{"location":"home/streamlit/widgets/#working-with-checkbox","text":"if st . checkbox ( \"Show/Hide\" ): st . text ( \"Showing something\" )","title":"Working with Checkbox"},{"location":"home/streamlit/widgets/#working-with-expander","text":"if st . expander ( \"Python\" ): st . success ( \"Hello Python\" ) with st . expander ( \"Julia\" ): st . text ( \"hello Julia\" )","title":"Working with Expander"},{"location":"home/streamlit/widgets/#working-with-one-select","text":"my_lang = [ \"Python\" , \"Julia\" , \"Go\" , \"Rust\" ] choice = st . selectbox ( \"Language\" , my_lang ) st . write ( \"You selected {} \" . format ( choice ))","title":"Working with one select"},{"location":"home/streamlit/widgets/#working-with-multiple-select","text":"spoken_lang = ( \"English\" , \"French\" , \"Spanish\" , \"Twi\" ) my_spoken_lang = st . multiselect ( \"Spoken Lang\" , spoken_lang , default = \"English\" )","title":"Working with multiple select"},{"location":"home/streamlit/widgets/#working-with-slider","text":"The difference between st.slider and st.select_slider is that slider only accepts numerical or date/time data and takes a range as input, while select_slider accepts any datatype and takes an iterable set of options. Numbers (Int/Float/Dates) age = st . slider ( \"Age\" , 1 , 100 ) st . write ( \"age:\" , age ) Any Datatype using Select Slider color = st . select_slider ( \"Choose Color\" , options = [ \"yellow\" , \"red\" , \"blue\" , \"black\" , \"white\" ], value = ( \"yellow\" , \"red\" )) st . write ( \"color:\" , color )","title":"Working with slider"}]}